{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used a  modified version of a  script from this github comment to chunk the text into `max_length` bits and then used max pooling to create a single embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_poems = load_dataset(\"parquet\", data_files=\"data/all-poems-active-clean-alphanumeric-for-chunking.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['poem_id', 'cleaned_content', 'text_for_embedding'],\n",
       "        num_rows: 16807\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[all_poems['train']['poem_id'] == 27139]['cleaned_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems = all_poems[\"train\"][\"cleaned_content\"]\n",
    "corpus = all_poems[\"train\"][\"text_for_embedding\"]\n",
    "poem_ids= all_poems[\"train\"][\"poem_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.34 s (started: 2023-10-25 13:49:50 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "# highest performing for assymetric search and normalized\n",
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetTokenizerFast(name_or_path='/home/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2/', vocab_size=30527, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t104: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30526: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.43 ms (started: 2023-10-25 07:50:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.1 ms (started: 2023-10-25 18:03:33 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# from : https://github.com/simonw/llm-sentence-transformers/issues/8#issuecomment-1732618592\n",
    "# sbert silently crops any token above the max_seq_length,\n",
    "# so we do a windowing embedding then sum. The normalization happens\n",
    "# afterwards.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def encode_sentences(sentences, modelName):\n",
    "    \n",
    "    #max_len = modelName.max_seq_length()\n",
    "    max_len = 2048\n",
    "    encode = modelName.encode\n",
    "    \n",
    "    assert isinstance(max_len, int), \"n must be int\"\n",
    "    n23 = (max_len * 2) // 3\n",
    "    add_sent = []  # additional sentences\n",
    "    add_sent_idx = []  # indices to keep track of sub sentences\n",
    "\n",
    "    for i, s in enumerate(sentences):\n",
    "        # skip if the sentence is short\n",
    "\n",
    "        length = len(encode(s))\n",
    "        if length <= max_len:\n",
    "            continue\n",
    "\n",
    "        # otherwise, split the sentence at regular interval\n",
    "        # then do the embedding of each\n",
    "        # and finally maxpool those sub embeddings together\n",
    "        # the renormalization happens later in the code\n",
    "        \n",
    "        sub_sentences = []\n",
    "        words = s.split(\" \")\n",
    "        avg_tkn = length / len(words)\n",
    "        # start at 90% of the supposed max_len\n",
    "        j = int(max_len / avg_tkn * 0.8)\n",
    "        while len(encode(\" \".join(words))) > max_len:\n",
    "            # if reached max length, use that minus one word\n",
    "            until_j = len(encode(\" \".join(words[:j])))\n",
    "            if until_j >= max_len:\n",
    "                jjj = 1\n",
    "                while len(encode(\" \".join(words[: j - jjj]))) >= max_len:\n",
    "                    jjj += 1\n",
    "                sub_sentences.append(\" \".join(words[: j - jjj]))\n",
    "\n",
    "                # remove first word until 1/3 of the max_token was removed\n",
    "                # this way we have a rolling window\n",
    "                jj = int((max_len // 3) / avg_tkn * 0.8)\n",
    "                while len(encode(\" \".join(words[jj: j - jjj]))) > n23:\n",
    "                    jj += 1\n",
    "                words = words[jj:]\n",
    "\n",
    "                j = int(max_len / avg_tkn * 0.8)\n",
    "            else:\n",
    "                diff = abs(max_len - until_j)\n",
    "                if diff > 10:\n",
    "                    j += int(10 / avg_tkn)\n",
    "                else:\n",
    "                    j += 1\n",
    "\n",
    "        sub_sentences.append(\" \".join(words))\n",
    "\n",
    "        sentences[i] = \" \"  # discard this sentence as we will keep only\n",
    "        # the sub sentences maxpooled\n",
    "\n",
    "        # remove empty text just in case\n",
    "        if \"\" in sub_sentences:\n",
    "            while \"\" in sub_sentences:\n",
    "                sub_sentences.remove(\"\")\n",
    "        assert (\n",
    "            sum([len(encode(ss)) > max_len for ss in sub_sentences]) == 0\n",
    "        ), f\"error when splitting long sentences: {sub_sentences}\"\n",
    "        add_sent.extend(sub_sentences)\n",
    "        add_sent_idx.extend([i] * len(sub_sentences))\n",
    "\n",
    "    if add_sent:\n",
    "        sent_check = [len(encode(s)) > max_len for s in sentences]\n",
    "        addsent_check = [len(encode(s)) > max_len for s in add_sent]\n",
    "        assert (\n",
    "            sum(sent_check + addsent_check) == 0\n",
    "        ), f\"The rolling average failed apparently:\\n{sent_check}\\n{addsent_check}\"\n",
    "    vectors = modelName.encode(\n",
    "        sentences=sentences + add_sent,\n",
    "        show_progress_bar=True,\n",
    "        output_value=\"sentence_embedding\",\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=False,\n",
    "    )\n",
    "\n",
    "    if add_sent:\n",
    "        # at the position of the original sentence (not split)\n",
    "        # add the vectors of the corresponding sub_sentence\n",
    "        # then return only the 'maxpooled' section\n",
    "        assert len(add_sent) == len(add_sent_idx), \"Invalid add_sent length\"\n",
    "        offset = len(sentences)\n",
    "        for sid in list(set(add_sent_idx)):\n",
    "            id_range = [i for i, j in enumerate(add_sent_idx) if j == sid]\n",
    "            add_sent_vec = vectors[offset +\n",
    "                                   min(id_range): offset + max(id_range), :]\n",
    "            vectors[sid] = np.amax(add_sent_vec, axis=0)\n",
    "        return vectors[:offset]\n",
    "    else:\n",
    "        return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embedding = encode_sentences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16807, 768)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.19 ms (started: 2023-10-25 11:02:11 +00:00)\n"
     ]
    }
   ],
   "source": [
    "corpus_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.16 ms (started: 2023-10-25 11:09:40 +00:00)\n"
     ]
    }
   ],
   "source": [
    "corpus_embedding_cache_path = \"data/all-poems-active-clean-max-pooled-alphanuemric-corpus-embeddings-{}.pkl\".format(\n",
    "    \"all-mpnet-base-v2\".replace(\"/\", \"-\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 160 ms (started: 2023-10-25 11:11:17 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# DONT RUN THIS ON RERUNS GENIUS!!  YOU ARE WRITING THE FILE\n",
    "import pickle\n",
    "\n",
    "with open(corpus_embedding_cache_path, \"wb\") as fOut:\n",
    "    pickle.dump({\"poem_id\": poem_ids , \"poem\": poems, \"embedding\": corpus_embedding}, fOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.39 s (started: 2023-10-25 18:53:18 +00:00)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModel\n",
    "from numpy.linalg import norm\n",
    "\n",
    "jina = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True) # trust_remote_code is needed to use the encode method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jina_embedding_chunked = encode_sentences(corpus, jina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT RUN THIS ON RERUNS GENIUS YOU ARE WRITING\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('data/all-poems-AC-AN-2048-chunked-jina-v2-base-en-embeddings.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({\"poem_id\": poem_ids , \"poem\": poems, \"embedding\": jina_embedding_chunked}, fOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jina_embeddings= jina.encode(corpus, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 146 ms (started: 2023-10-25 17:59:02 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# DONT RUN THIS ON RERUNS GENIUS YOU ARE WRITING\n",
    "import pickle\n",
    "\n",
    "with open('data/all-poems-AC-AN-jina-v2-base-en', \"wb\") as fOut:\n",
    "    pickle.dump({\"poem_id\": poem_ids , \"poem\": poems, \"embedding\": jina_embeddings}, fOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jina_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.63 s (started: 2023-10-25 13:45:14 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Create NGT index for the chunked corpus embedded on Jina\n",
    "\n",
    "EMB_SIZE = 768\n",
    "import ngtpy\n",
    "\n",
    "# Initialize index name\n",
    "corpus_ix_name = \"indices/ngt_AP-AC-AN-chunked-embeddings-{}\".format(\n",
    "    \"all-mpnet-base-v2\".replace(\"/\", \"-\")\n",
    ")\n",
    "\n",
    "# Create an empty index\n",
    "\n",
    "ngtpy.create(\n",
    "    path=bytes(corpus_ix_name, encoding=\"utf8\"),\n",
    "    dimension=EMB_SIZE,\n",
    "    distance_type=\"Normalized Cosine\",\n",
    ")\n",
    "\n",
    "ngt_corpus_index = ngtpy.Index(bytes(corpus_ix_name, encoding=\"utf8\"))\n",
    "\n",
    "# insert the objects\n",
    "\n",
    "ngt_corpus_index.batch_insert(corpus_embedding)\n",
    "\n",
    "# save the index.\n",
    "ngt_corpus_index.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.61 s (started: 2023-10-25 13:47:19 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Create NGT index for the un-chunked corpus embedded on Jina\n",
    "\n",
    "\n",
    "EMB_SIZE = 768\n",
    "import ngtpy\n",
    "\n",
    "# Initialize index name\n",
    "corpus_ix_name = \"indices/ngt_AP-AC-AN-chunked-embeddings-jina-embeddings-v2-base-en\"\n",
    "\n",
    "\n",
    "# Create an empty index\n",
    "\n",
    "ngtpy.create(\n",
    "    path=bytes(corpus_ix_name, encoding=\"utf8\"),\n",
    "    dimension=EMB_SIZE,\n",
    "    distance_type=\"Normalized Cosine\",\n",
    ")\n",
    "\n",
    "jina_corpus_index = ngtpy.Index(bytes(corpus_ix_name, encoding=\"utf8\"))\n",
    "\n",
    "# insert the objects\n",
    "\n",
    "jina_corpus_index.batch_insert(corpus_embedding)\n",
    "\n",
    "# save the index.\n",
    "jina_corpus_index.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"Rosy mornings, sad evenings\"]\n",
    "query_embedding = model.encode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_results = ngt_corpus_index.search(query_embedding, size=5)\n",
    "\n",
    "print(\"ID\\tDistance\")\n",
    "for result in corpus_results:\n",
    "    print(\"{}\\t{}\".format(*result))\n",
    "print(\n",
    "    \"# of distance computations=\"\n",
    "    + str(ngt_corpus_index.get_num_of_distance_computations())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results for the two indices were almost identical, so i've decided to junk the chunked embeddings and uses jina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
